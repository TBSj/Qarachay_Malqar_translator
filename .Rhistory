# 1. Libraries and consts             ####
library(keras)
library(tensorflow)
library(tfdatasets)
library(readr)
library(dplyr)
library(data.table)
library(this.path)
library(tokenizers.bpe)
library(stringr)
library(reticulate)
library(telegram.bot.dt)
FILEPATH_SOURCE_PREPARED <- str_replace(this.dir(), strsplit(this.dir(), '/')[[1]] %>% last(), "Source_prepared")
# FILEPATH_SOURCE_PREPARED <- str_replace(this.dir(), strsplit(this.dir(), '/')[[1]] %>% last(), "Source_prepared")
FILEPATH_SOURCE_PREPARED <- "1.Data"
FILE_TEXT <- file.path(FILEPATH_SOURCE_PREPARED, "Bary_bir_to_model.csv")
SRC_LANG <- 'rus'
TRG_LANG <- 'krc'
this.dir()
strsplit(this.dir(), '/')[[1]] %>% last()
# MODEL_PATH_SRC_BPE    <- file.path(this.dir(), 'BPE', paste0("youtokentome_", SRC_LANG, ".bpe"))
# MODEL_PATH_TARGET_BPE <- file.path(this.dir(), 'BPE', paste0("youtokentome_", TRG_LANG, ".bpe"))
MODEL_DIR_BPE <- str_replace(this.dir(), 'Qarachay_Malqar_translator/2.R', "Model")
MODEL_PATH_SRC_BPE    <- file.path(this.dir(), 'BPE', paste0("youtokentome_", SRC_LANG, ".bpe"))
MODEL_PATH_SRC_BPE
this.dir()
MODEL_DIR_BPE
MODEL_PATH_SRC_BPE    <- file.path(MODEL_DIR_BPE, 'BPE', paste0("youtokentome_", SRC_LANG, ".bpe"))
MODEL_PATH_TARGET_BPE <- file.path(MODEL_DIR_BPE, 'BPE', paste0("youtokentome_", TRG_LANG, ".bpe"))
MODEL_PATH_SRC_BPE
MODEL_PATH_TARGET_BPE
TRANSFORMER_NAME <- paste0('transformer_', SRC_LANG, '_to_',TRG_LANG, '.h5')
# MODEL_PATH_SRC_BPE    <- file.path(this.dir(), 'BPE', paste0("youtokentome_", SRC_LANG, ".bpe"))
# MODEL_PATH_TARGET_BPE <- file.path(this.dir(), 'BPE', paste0("youtokentome_", TRG_LANG, ".bpe"))
MODEL_DIR <- str_replace(this.dir(), 'Qarachay_Malqar_translator/2.R', "Model")
MODEL_PATH_SRC_BPE    <- file.path(MODEL_DIR, 'BPE', paste0("youtokentome_", SRC_LANG, ".bpe"))
MODEL_PATH_TARGET_BPE <- file.path(MODEL_DIR, 'BPE', paste0("youtokentome_", TRG_LANG, ".bpe"))
TRANSFORMER_FILE
TRANSFORMER_NAME
# TRANSFORMER_FILE <- file.path(this.dir(), 'model', TRANSFORMER_NAME)
TRANSFORMER_FILE <- file.path(MODEL_DIR, TRANSFORMER_NAME)
TRANSFORMER_NAME_ALL <- paste0('transformer_', SRC_LANG, '_to_',TRG_LANG, '_all.h5')
TRANSFORMER_NAME_ALL
TRANSFORMER_FILE_ALL <- file.path(MODEL_DIR, TRANSFORMER_NAME_ALL)
# 2. Preparing                        ####
#    2.1. Download dataset            ####
text_pairs <- FILE_TEXT %>%
fread()
str(text_pairs[sample(nrow(text_pairs), 1), ])
model_src_bpe <- bpe_load_model(MODEL_PATH_SRC_BPE)
model_trg_bpe <- bpe_load_model(MODEL_PATH_TARGET_BPE)
MODEL_PATH_SRC_BPE
# MODEL_PATH_SRC_BPE    <- file.path(this.dir(), 'BPE', paste0("youtokentome_", SRC_LANG, ".bpe"))
# MODEL_PATH_TARGET_BPE <- file.path(this.dir(), 'BPE', paste0("youtokentome_", TRG_LANG, ".bpe"))
MODEL_DIR <- str_replace(this.dir(), 'Qarachay_Malqar_translator/2.R', "Models")
MODEL_PATH_SRC_BPE    <- file.path(MODEL_DIR, 'BPE', paste0("youtokentome_", SRC_LANG, ".bpe"))
MODEL_PATH_TARGET_BPE <- file.path(MODEL_DIR, 'BPE', paste0("youtokentome_", TRG_LANG, ".bpe"))
TRANSFORMER_NAME <- paste0('transformer_', SRC_LANG, '_to_',TRG_LANG, '.h5')
# TRANSFORMER_FILE <- file.path(this.dir(), 'model', TRANSFORMER_NAME)
TRANSFORMER_FILE <- file.path(MODEL_DIR, TRANSFORMER_NAME)
TRANSFORMER_NAME_ALL <- paste0('transformer_', SRC_LANG, '_to_',TRG_LANG, '_all.h5')
TRANSFORMER_FILE_ALL <- file.path(MODEL_DIR, TRANSFORMER_NAME_ALL)
# 2. Preparing                        ####
#    2.1. Download dataset            ####
text_pairs <- FILE_TEXT %>%
fread()
str(text_pairs[sample(nrow(text_pairs), 1), ])
model_src_bpe <- bpe_load_model(MODEL_PATH_SRC_BPE)
model_trg_bpe <- bpe_load_model(MODEL_PATH_TARGET_BPE)
src_diglist <- bpe_encode(model = model_src_bpe,
x     = text_pairs[[SRC_LANG]],
type  = "ids",
bos   = TRUE,
eos   = TRUE)
src_maxlen <- lapply(src_diglist, length) %>% unlist() %>% max()
trg_diglist <- bpe_encode(model = model_trg_bpe,
x     = text_pairs[[TRG_LANG]],
type  = "ids",
bos   = TRUE,
eos   = TRUE)
trg_maxlen <- lapply(trg_diglist, length) %>% unlist() %>% max()
sequence_length <- max(trg_maxlen, src_maxlen) # 152
src_matrix <-
pad_sequences(src_diglist, maxlen = sequence_length,  padding = "post")
trg_matrix <-
pad_sequences(trg_diglist, maxlen = sequence_length + 1, padding = "post")
#    2.3. Train-test-split            ####
num_test_samples <- 10
num_val_samples <- round(0.2 * nrow(text_pairs))
num_train_samples <- nrow(text_pairs) - num_val_samples - num_test_samples
pair_group <- sample(base::c(
rep("train", num_train_samples),
rep("test", num_test_samples),
rep("val", num_val_samples)
))
test_pairs <- text_pairs[pair_group == "test", ]
x_train <- src_matrix[pair_group == "train",]
y_train <- trg_matrix[pair_group == "train",]
x_valid <- src_matrix[pair_group == "val",]
y_valid <- trg_matrix[pair_group == "val",]
src_vocab_size <- model_src_bpe$vocab_size # 46962
trg_vocab_size <- model_trg_bpe$vocab_size # 40520
dropout_rate <- 0.4 # Дропаут
embed_dim <- 256  # Количество нейронов в слое (или глубина внимания) (Эм иги 512, компьютерим тарталмайды)
dense_dim <- 1024 #  Расстояние позиций (токенов) для слоя позиционно-зависимого прямого распространения (Эм иги 2048, компьютерим тарталмайды)
num_heads <- 8 # Кол-во голов внимания (8)
buffer_size <- nrow(x_train)
learning_rate  <-  1e-2 # 1e-4
epoches <- 5
ep_stop <- 2
batch_size <- 8 # 64
regul <- regularizer_l1_l2(l1 = learning_rate, l2 = learning_rate)
#    2.5. Preparing matrix tf         ####
# Слой векторизации можно вызывать как с пакетными, так и с непакетными данными. Здесь мы применяем векторизацию перед пакетной обработкой данных
format_pair <- function(pair) {
src_p <- pair$src %>% as_tensor(dtype = 'int64')
trg_p <- pair$trg %>% as_tensor(dtype = 'int64')
# Опускаем последний токен в испанском предложении, чтобы входные данные и цели имели одинаковую длину. [NA:–2] удаляет последний элемент тензора
inputs <- list()
inputs[[SRC_LANG]] = src_p
inputs[[TRG_LANG]] = trg_p[NA:-2]
# [2:NA] удаляет первый элемент тензора
targets <- trg_p[2:NA]
# Целевое испанское предложение на один шаг впереди.
# Оба имеют одинаковую длину (20 слов)
list(inputs, targets)
}
train_ds <- tensor_slices_dataset(keras_array(list(src = x_train, trg = y_train))) %>%
dataset_map(format_pair) %>%
dataset_shuffle(buffer_size = buffer_size) %>%
dataset_batch(batch_size)
val_ds <- tensor_slices_dataset(keras_array(list(src = x_valid, trg = y_valid))) %>%
dataset_map(format_pair) %>%
dataset_shuffle(buffer_size = buffer_size) %>%
dataset_batch(batch_size)
MobiDickMessage <- function(text = 'юйренибди', chat_id = '428576415', token = "5195278336:AAEn7oQyXox7CQFzNpQHnjNqqC1KKBU3MgY"){
bot <- Bot(token=token)
mess <- bot$sendMessage(chat_id = chat_id,
text = text,
parse_mode = "Markdown"
)
}
# 3. Preparing model                  ####
#    3.1. PositionalEmbedding         ####
layer_positional_embedding <- new_layer_class(
classname = "PositionalEmbedding",
initialize = function(sequence_length, # Недостатком позиционного встраивания является то, что длина последовательности должна быть известна заранее
input_dim, output_dim, ...) {
super$initialize(...)
self$token_embeddings <- # Подготовка layer_embedding() для индексов токенов
layer_embedding(input_dim = input_dim,
output_dim = output_dim)
# Подготовка layer_embedding() для позиций токенов
self$position_embeddings <-
layer_embedding(input_dim = sequence_length,
output_dim = output_dim)
self$sequence_length <- sequence_length
self$input_dim <- input_dim
self$output_dim <- output_dim
},
call = function(inputs) {
len <- tf$shape(inputs)[-1] # tf$shape(inputs)[–1] вырезает последний элемент формы (tf$shape() возвращает форму в виде тензора)
positions <-
tf$range(start = 0L, limit = len, delta = 1L) # tf$range() похож на seq() в R, создает целочисленную последовательность: [0, 1, 2, …, limit – 1]
embedded_tokens <- self$token_embeddings(inputs)
embedded_positions <- self$position_embeddings(positions)
embedded_tokens + embedded_positions # Складываем векторы встраивания
},
# Как и layer_embedding(), этот слой должен уметь генерировать маску, чтобы мы могли игнорировать заполнение нулями во входных данных. Метод calculate_mask() будет автоматически вызван фреймворком, и маска распространится на следующий уровень
compute_mask = function(inputs, mask = NULL) {
inputs != 0
},
# Сериализация для сохранения модели
get_config = function() {
config <- super$get_config()
for(name in base::c("output_dim", "sequence_length", "input_dim"))
config[[name]] <- self[[name]]
config
}
)
#    3.2. Transformer Encoder layer   ####
layer_transformer_encoder <- new_layer_class(
classname = "TransformerEncoderLayer",
# initialize = function(embed_dim, dense_dim, num_heads, dropout = 0.3, ...) {
initialize = function(embed_dim, dense_dim, num_heads, ...) {
super$initialize(...)
self$embed_dim <- embed_dim # Размер векторов входных токенов
self$dense_dim <- dense_dim # Размер внутреннего слоя Dense
self$num_heads <- num_heads # Количество голов внимания
self$attention <-
layer_multi_head_attention(num_heads = num_heads,
key_dim = embed_dim)
self$dense_proj <- keras_model_sequential() %>%
layer_dense(dense_dim, activation = "relu") %>%
# layer_dropout(dropout) %>%
layer_dropout(dropout_rate) %>%
layer_dense(embed_dim)
self$layernorm_1 <- layer_layer_normalization()
self$layernorm_2 <- layer_layer_normalization()
},
call = function(inputs, mask = NULL) {
# Маска, которая будет сгенерирована слоем встраивания, будет двумерной, но слой внимания ожидает, что она будет трех- или четырехмерной, поэтому мы расширяем ее размерность
if (!is.null(mask))
mask <- mask[, tf$newaxis, ]
inputs %>%
{ self$attention(., ., attention_mask = mask) + . } %>% # Добавляем остаточную связь к выходу слоя dense_proj()
self$layernorm_1() %>%
{ self$dense_proj(.) + . } %>% # Добавляем остаточную связь к выходу слоя внимания
self$layernorm_2()
},
# Выполняем сериализацию, благодаря чему можно сохранить модель
get_config = function() {
config <- super$get_config()
for(name in base::c("embed_dim", "num_heads", "dense_dim"))
config[[name]] <- self[[name]]
config
}
)
# layer_transformer_encoder_tf_hub <- hub$KerasLayer(PATH_TF_HUB, trainable = TRUE)
#    3.3. Transformer Decoder layer   ####
layer_transformer_decoder <- new_layer_class(
classname = "TransformerDecoderLayer",
# initialize = function(embed_dim, dense_dim, num_heads, dropout = 0.3, ...) {
initialize = function(embed_dim, dense_dim, num_heads, ...) {
super$initialize(...)
self$embed_dim <- embed_dim
self$dense_dim <- dense_dim
self$num_heads <- num_heads
self$attention_1 <- layer_multi_head_attention(num_heads = num_heads,
key_dim = embed_dim)
self$attention_2 <- layer_multi_head_attention(num_heads = num_heads,
key_dim = embed_dim)
self$dense_proj <- keras_model_sequential() %>%
layer_dense(dense_dim, activation = "relu") %>%
# layer_dropout(dropout) %>%
layer_dropout(dropout_rate) %>%
layer_dense(embed_dim)
self$layernorm_1 <- layer_layer_normalization()
self$layernorm_2 <- layer_layer_normalization()
self$layernorm_3 <- layer_layer_normalization()
self$supports_masking <- TRUE # Этот атрибут гарантирует, что слой будет распространять свою входную маску на свои выходные данные; маскировка в Keras явно включена. Если вы передаете маску слою, который не реализует compute_mask() и не предоставляет этот атрибут supports_masking, это ошибка
},
get_config = function() {
config <- super$get_config()
for (name in base::c("embed_dim", "num_heads", "dense_dim"))
config[[name]] <- self[[name]]
config
},
get_causal_attention_mask = function(inputs) {
c(batch_size, sequence_length, .) %<-% # Третья ось — encoding_length; мы не используем ее здесь
tf$unstack(tf$shape(inputs))
x <- tf$range(sequence_length) # Целочисленная последовательность [0, 1, 2, … sequence_length–1]
i <- x[, tf$newaxis]
j <- x[tf$newaxis, ]
# mask представляет собой квадратную матрицу формы (sequence_length, sequence_length), с 1 в нижнем треугольнике и 0 в остальных местах. Например, если sequence_length равно 4
mask <- tf$cast(i >= j, "int32") # Используем изменение размерности тензора в нашей операции >=. Приводим dtype bool к int32
tf$tile(mask[tf$newaxis, , ],
tf$stack(base::c(batch_size, 1L, 1L))) # Добавляем размер пакета в маску, а затем размещаем (rep()) вдоль оси пакета batch_size раз. Возвращенный тензор имеет форму (batch_size, sequence_length, sequence_length)
},
call = function(inputs, encoder_outputs, mask = NULL) {
causal_mask <- self$get_causal_attention_mask(inputs) # Получаем каузальную маску
# Маска, предоставляемая в вызове, является маской заполнения (она описывает места заполнения в целевой последовательности)
if (is.null(mask))
mask <- causal_mask
else
mask %<>% { tf$minimum(tf$cast(.[, tf$newaxis, ], "int32"),
causal_mask) } # Объединяем маску заполнения с каузальной маской
inputs %>%
{ self$attention_1(query = ., value = ., key = .,
attention_mask = causal_mask) + . } %>% # Передайте причинную маску первому слою внимания, который применяет самовнимание к целевой последовательности
self$layernorm_1() %>% # Выход attention_1() с добавленным остатком передается в layernorm_1()
{ self$attention_2(query = .,
value = encoder_outputs, # Используем encoder_outputs, предоставленные в вызове, в качестве значения и ключа для warning_2()
key = encoder_outputs,
attention_mask = mask) + . } %>% # Передаем комбинированную маску второму слою внимания, который связывает исходную последовательность с целевой последовательностью
self$layernorm_2() %>% # Выход attention_2() с добавленным остатком передается в layernorm_2()
{ self$dense_proj(.) + . } %>%
self$layernorm_3() # Выход dense_proj() с добавленным остатком передается в layernorm_3()
})
transformer <- load_model_hdf5(TRANSFORMER_FILE,
custom_objects =
list(PositionalEmbedding = layer_positional_embedding,
TransformerEncoderLayer = layer_transformer_encoder,
TransformerDecoderLayer = layer_transformer_decoder
# AdamWarmup = opt
)
)
transformer %>%
compile(optimizer =  keras$optimizers$Adam(learning_rate = learning_rate), #opt,#
loss = 'sparse_categorical_crossentropy',
metrics = 'acc')
callbacks_list <- list(
#
callback_early_stopping(
monitor = "val_loss",
patience = ep_stop), # Прерываем обучение, когда точность проверки перестанет улучшаться в течение двух эпох
callback_model_checkpoint(
filepath = TRANSFORMER_FILE,
monitor = "val_loss",
save_best_only = TRUE)
)
# 5. Save model                       ####
save_model_hdf5(transformer, filepath = TRANSFORMER_FILE_ALL)
transformer1 <- load_model_hdf5(TRANSFORMER_FILE_ALL,
custom_objects =
list(PositionalEmbedding = layer_positional_embedding,
TransformerEncoderLayer = layer_transformer_encoder,
TransformerDecoderLayer = layer_transformer_decoder,
AdamWarmup = opt
))
transformer1 <- load_model_hdf5(TRANSFORMER_FILE_ALL,
custom_objects =
list(PositionalEmbedding = layer_positional_embedding,
TransformerEncoderLayer = layer_transformer_encoder,
TransformerDecoderLayer = layer_transformer_decoder
# AdamWarmup = opt
))
y
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("lme4")
try(devtools::install_github("St-Digital-Twin/133.DtwinLib",
auth_token = "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"))
install.packages("mailR")
try(devtools::install_github("St-Digital-Twin/133.DtwinLib",
auth_token = "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"))
install.packages("plotly")
try(devtools::install_github("St-Digital-Twin/133.DtwinLib",
auth_token = "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"))
devtools::install_github("TBSj/telegram.bot.dt")
devtools::install_github("St-Digital-Twin/DTwinDW", auth_token = "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")
library(reticulate)
reticulate::install_python()
reticulate::install_miniconda()
library(tensorflow)
# 1. Libraries and consts             ####
library(keras)
install.packages("keras")
install.packages("tensorflow")
library(tensorflow)
install_tensorflow(version = 'gpu')
library(tensorflow)
install_tensorflow()
install_tensorflow()
library(tensorflow)
install_tensorflow()
library(tensorflow)
# 1. Libraries and consts             ####
library(keras)
tf$device()
tf$test$is_built_with_cuda
tf$test$is_built_with_cuda()
tf$test$is_gpu_available()
install_tensorflow(version = 'gpu')
library(tensorflow)
# 1. Libraries and consts             ####
library(keras)
install_tensorflow(version = 'gpu')
install_tensorflow(version = '2.10.1-gpu')
library(tensorflow)
install_tensorflow(version = '2.10.1-gpu')
# 1. Libraries and consts             ####
library(keras)
library(tensorflow)
install_tensorflow(version = '2.10.1-gpu')
# 1. Libraries and consts             ####
library(keras)
library(tensorflow)
tf$test$is_built_with_cuda
tf$test$is_built_with_cuda()
tf$test$is_gpu_available
tf$test$is_gpu_available()
tf$device('cpu')
tf$device('/cpu:0')
tf$test$is_gpu_available()
tf$test$Benchmark()
tf$test$Benchmark
tf$test$TestCase()
a = tf$test$TestCase()
a$test_session()
config = tf$config
config = tf$sysconfig
gpu_options = tf$compat$v1$GPUOptions(per_process_gpu_memory_fraction=0.3)
sess = tf$compat$v1$Session(tf$compat$v1$ConfigProto(gpu_options=gpu_options))
gpu_options
sess = tf$compat$v1$Session(config = tf$compat$v1$ConfigProto(gpu_options=gpu_options))
sess
sess$run()
tf$device()
tf$test$is_gpu_available()
gpu_options = tf$compat$v1$GPUOptions(per_process_gpu_memory_fraction=0.1)
sess = tf$compat$v1$Session(config = tf$compat$v1$ConfigProto(gpu_options=gpu_options))
tf$test$is_gpu_available()
tf.device('/cpu:0')
tf$device('/cpu:0')
tf$test$is_gpu_available()
sess$list_devices()
# 1. Libraries and consts             ####
library(keras)
library(tensorflow)
library(tfdatasets)
library(readr)
library(dplyr)
library(data.table)
library(this.path)
library(tokenizers.bpe)
library(stringr)
library(reticulate)
library(telegram.bot.dt)
# 1. Libraries and consts             ####
library(keras)
library(tensorflow)
library(tfdatasets)
library(readr)
library(dplyr)
library(data.table)
library(this.path)
library(tokenizers.bpe)
library(stringr)
library(reticulate)
library(telegram.bot.dt)
gpu_options = tf$compat$v1$GPUOptions(per_process_gpu_memory_fraction=0.9)
sess = tf$compat$v1$Session(config = tf$compat$v1$ConfigProto(gpu_options=gpu_options))
gpu_options = tf$compat$v1$GPUOptions(per_process_gpu_memory_fraction=0.98)
sess = tf$compat$v1$Session(config = tf$compat$v1$ConfigProto(gpu_options=gpu_options))
source("D:/Projects/R/R_projects/Qarachay_Malqar_translator/2.R/Transformer_learning.R")
gc()
