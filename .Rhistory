)
result
tokenizer.decode(result[0], skip_special_tokens=True)
nltk.translate.bleu(references=y_sent, hypothesis=pred_sent)
nltk.translate.bleu(references=y_sent, hypothesis=pred_sent)
for(i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
blue_val.append(nltk.translate.bleu(references=y_sent, hypothesis=pred_sent))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
blue_val
VAL_AMOUNT_FOR_TEST
i
random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)
for(i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
blue_val.append(nltk.translate.bleu(references=y_sent, hypothesis=pred_sent))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
chrf_losses
losses = []
val_losses = []
blue_val = []
chrf_losses = []
losses.append(loss.item())
for(i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
blue_val.append(nltk.translate.bleu(references=y_sent, hypothesis=pred_sent))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
blue_val
chrf_losses
for(i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
# blue_val.append(nltk.translate.bleu(references=y_sent, hypothesis=pred_sent))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
chrf_losses
chrf_losses
val_losses = []
blue_val = []
chrf_losses = []
del i
for(i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
# blue_val.append(nltk.translate.bleu(references=y_sent, hypothesis=pred_sent))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
chrf_losses
for(i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
losses = []
val_losses = []
blue_val = []
chrf_losses = []
for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
blue_val.append(nltk.translate.bleu(references=y_sent, hypothesis=pred_sent))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
blue_val
chrf_losses
train_loss = np.mean(losses[-report_steps:])
val_loss = np.mean(val_losses)
val_blue = np.mean(blue_val)
val_chrf = np.mean(chrf_losses)
train_loss
losses
losses.append(loss.item())
train_loss = np.mean(losses[-report_steps:])
val_loss = np.mean(val_losses)
val_blue = np.mean(blue_val)
val_chrf = np.mean(chrf_losses)
val_loss
val_chrf
val_blue
val_losses
np.exp(losses[-report_steps:])
train_perp = np.mean(np.exp(losses[-report_steps:]))
train_perp
val_perp = np.mean(np.exp(val_losses))
val_perp
val_loss
print('Step', i, 'loss', np.mean(losses[-report_steps:]))
oss:', val_loss,
'; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:' val_chrf))
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss,
'; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:' val_chrf))
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss, '; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:' val_chrf))
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss, '; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:' val_chrf)
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss,
'; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:' val_chrf)
train_loss
val_loss
train_perp
val_perp
val_blue
val_chrf
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss,
'; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:' val_chrf)
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss
)
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss,
'; perplexity:', train_perp
)
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss,
'; perplexity:', train_perp, '; validation perplexity:', val_perp
)
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss,
'; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue
)
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss,
'; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:' val_chrf
'; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:', val_chrf)
print('Step:', i, '; loss:', train_loss, '; validation loss:', val_loss,
'; perplexity:', train_perp, '; validation perplexity:', val_perp, '; BLUE:', val_blue, '; CHRF:', val_chrf)
val_blue
val_losses = []
blue_val = []
chrf_losses = []
for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
blue_val.append(nltk.translate.bleu(references=y_sent, hypothesis=pred_sent))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
blue_val.append(nltk.translate.bleu(references=y_sent, hypothesis=pred_sent))
nltk.translate.bleu(references=y_sent, hypothesis=pred_sent)
nltk.translate.bleu(references=y_sent, hypothesis=pred_sent)
y_sent
pred_sent
nltk.translate.bleu_score.corpus_bleu(references=y_sent, hypothesis=pred_sent)
nltk.translate.bleu_score.corpus_bleu(reference=y_sent, hypothesis=pred_sent)
nltk.translate.bleu_score.corpus_bleu(list_of_references=y_sent, hypotheses=pred_sent)
pred_sent
nltk.translate.bleu_score.corpus_bleu(list_of_references=y_sent, pred_sent)
nltk.translate.blue(references=y_sent, hypothesis=pred_sent, smoothing_function=SmoothingFunction())
nltk.translate.bleu(references=y_sent, hypothesis=pred_sent, smoothing_function=SmoothingFunction())
SmoothingFunction()
y_sent.split(" ")
nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" "))
nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent)
nltk.translate.bleu_score.sentence_bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" "))
nltk.translate.bleu_score.sentence_bleu(references=[y_sent.split(" ")], hypothesis=pred_sent.split(" "))
nltk.translate.bleu_score.sentence_bleu(references=[y_sent.split(" ")], hypothesis=pred_sent.split(" "))
nltk.translate.bleu_score.sentence_bleu(references=[y_sent.split(" ")], hypothesis=pred_sent.split(" "))
train_loss = np.mean(losses[-report_steps:])
nltk.translate.bleu_score.sentence_bleu(references=[y_sent.split(" ")], hypothesis=pred_sent.split(" "))
nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" "))
nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" "))
nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" "))
nltk.translate.bleu_score.sentence_bleu(references=[y_sent], hypothesis=pred_sent)
nltk.translate.bleu_score.sentence_bleu(references=[y_sent], hypothesis=pred_sent)
nltk.translate.bleu_score.sentence_bleu(references=[y_sent], hypothesis=pred_sent)
nltk.translate.bleu_score.sentence_bleu(references=[y_sent], hypothesis=pred_sent)
nltk.translate.bleu_score.sentence_bleu(references=[y_sent], hypothesis=pred_sent)
nltk.translate.bleu_score.sentence_bleu(references=[y_sent.split(" ")], hypothesis=pred_sent.split(" "))
nltk.translate.bleu_score.sentence_bleu(references=[y_sent.split(" ")], hypothesis=pred_sent.split(" "))
nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" "))
nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" "))
nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" "))
val_losses = []
blue_val = []
chrf_losses = []
for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
blue_val.append(nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" ")))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
blue_val
val_losses = []
blue_val = []
chrf_losses = []
for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST):
# vals = [valid_pairs[i] for i in random.sample(range(0, len(valid_pairs)), VAL_AMOUNT_FOR_TEST)]
vals = valid_pairs[i]
x_sent = vals[SRC_LANG_DF_INDEX]
y_sent = vals[TRG_LANG_DF_INDEX]
x_val = tokenizer(x_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
y_val = tokenizer(y_sent, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)
model_out = model(
input_ids=x_val.input_ids,
attention_mask=x_val.attention_mask,
labels=y_val.input_ids,
decoder_attention_mask=y_val.attention_mask,
# return_dict=True
)
result = model.generate(
**x_val,
forced_bos_token_id=tokenizer.convert_tokens_to_ids(TRG_LANG)
)
val_losses.append(model_out.loss.item())
pred_sent = tokenizer.decode(result[0], skip_special_tokens=True)
# blue_val.append(nltk.translate.bleu(references=y_sent.split(" "), hypothesis=pred_sent.split(" ")))
blue_val.append(nltk.translate.bleu_score.sentence_bleu(references=[y_sent.split(" ")], hypothesis=pred_sent.split(" ")))
chrf_losses.append(nltk.translate.chrf(reference=y_sent, hypothesis=pred_sent))
blue_val
blue_val
chrf_losses
val_blue = np.mean(blue_val)
val_blue
y_sent
pred_sent
reticulate::repl_python()
from datasets import load_dataset
!pip install sacrebleu
!pip install sacrebleu
import evaluate
metric = evaluate.load("sacrebleu")
from datasets import load_dataset
books = load_dataset("opus_books", "en-fr")
books = books["train"].train_test_split(test_size=0.2)
books["train"][0]
checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
source_lang = "en"
target_lang = "fr"
prefix = "translate English to French: "
def preprocess_function(examples):
inputs = [prefix + example[source_lang] for example in examples["translation"]]
targets = [example[target_lang] for example in examples["translation"]]
model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
return model_inputs
tokenized_books = books.map(preprocess_function, batched=True)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_books = books.map(preprocess_function, batched=True)
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
import numpy as np
def postprocess_text(preds, labels):
preds = [pred.strip() for pred in preds]
labels = [[label.strip()] for label in labels]
return preds, labels
def compute_metrics(eval_preds):
preds, labels = eval_preds
if isinstance(preds, tuple):
preds = preds[0]
decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
result = metric.compute(predictions=decoded_preds, references=decoded_labels)
result = {"bleu": result["score"]}
prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
result["gen_len"] = np.mean(prediction_lens)
result = {k: round(v, 4) for k, v in result.items()}
return result
from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
training_args = Seq2SeqTrainingArguments(
output_dir="my_awesome_opus_books_model",
evaluation_strategy="epoch",
learning_rate=2e-5,
per_device_train_batch_size=16,
per_device_eval_batch_size=16,
weight_decay=0.01,
save_total_limit=3,
num_train_epochs=2,
predict_with_generate=True,
fp16=True,
# push_to_hub=True,
)
!pip install accelerate -U
# !pip install accelerate -U
training_args = Seq2SeqTrainingArguments(
output_dir="my_awesome_opus_books_model",
evaluation_strategy="epoch",
learning_rate=2e-5,
per_device_train_batch_size=16,
per_device_eval_batch_size=16,
weight_decay=0.01,
save_total_limit=3,
num_train_epochs=2,
predict_with_generate=True,
fp16=True,
# push_to_hub=True,
)
!pip install transformers[torch]
training_args = Seq2SeqTrainingArguments(
output_dir="my_awesome_opus_books_model",
evaluation_strategy="epoch",
learning_rate=2e-5,
per_device_train_batch_size=16,
per_device_eval_batch_size=16,
weight_decay=0.01,
save_total_limit=3,
num_train_epochs=2,
predict_with_generate=True,
fp16=True,
# push_to_hub=True,
)
library(magrittr)
library(data.table)
library(reticulate)
library(this.path)
library(jsonify)
library(qs)
library(readxl)
library(stringr)
# Sys.setenv(RETICULATE_PYTHON ="/usr/bin/python3")
reticulate::use_virtualenv('r-reticulate', required = TRUE)
# py_install('langchain', pip = TRUE)
# pip install faiss
# py_install('faiss-cpu', pip = TRUE)
# reticulate::virtualenv_install('r-reticulate', packages = c('langchain', 'openai', 'faiss-cpu', 'tiktoken', 'pandas'), ignore_installed=TRUE, version = '3.10')
langchain <- import('langchain')
# path_save_index <- 'C:/Users/User/Desktop/index1'
PATH <- this.dir()
OPENAI_API_KEY <- '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'
PATH_SAVE_INDEX <- file.path(PATH, "index_ru")
# Sys.setenv(RETICULATE_PYTHON ="/usr/bin/python3")
reticulate::use_virtualenv('r-reticulate', required = TRUE)
# py_install('langchain', pip = TRUE)
# pip install faiss
# py_install('faiss-cpu', pip = TRUE)
# reticulate::virtualenv_install('r-reticulate', packages = c('langchain', 'openai', 'faiss-cpu', 'tiktoken', 'pandas'), ignore_installed=TRUE, version = '3.10')
langchain <- import('langchain')
# path_save_index <- 'C:/Users/User/Desktop/index1'
PATH <- this.dir()
OPENAI_API_KEY <- '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'
PATH_SAVE_INDEX <- file.path(PATH, "index_ru")
index <- loadIndexLCh(langchain = langchain, OPENAI_API_KEY = OPENAI_API_KEY, path_save_index = PATH_SAVE_INDEX)
# py_install('langchain', pip = TRUE)
# pip install faiss
# py_install('faiss-cpu', pip = TRUE)
# reticulate::virtualenv_install('r-reticulate', packages = c('langchain', 'openai', 'faiss-cpu', 'tiktoken', 'pandas'), ignore_installed=TRUE, version = '3.10')
langchain <- import('langchain')
# path_save_index <- 'C:/Users/User/Desktop/index1'
PATH <- this.dir()
OPENAI_API_KEY <- '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'
PATH_SAVE_INDEX <- file.path(PATH, "index_ru")
prepareIndexLCh <- function(data,
page_content_column,
langchain,
path_save_index,
OPENAI_API_KEY,
chunk_size = 1000 # Максимальная длина символов
){
loader = langchain$document_loaders$DataFrameLoader(data, page_content_column=page_content_column)
documents = loader$load()
# создаем сплиттер документов, чтобы уложиться в лимит по токенам, в нашем случае это не очень полезный шаг
text_splitter = langchain$text_splitter$RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)
texts = text_splitter$split_documents(documents)
# задаем векторайзер
embeddings = langchain$embeddings$OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
# создаем хранилище
db = langchain$vectorstores$FAISS$from_documents(texts, embeddings)
# db$as_retriever()
# также можно сохранить хранилище локально
db$save_local(path_save_index)
}
loadIndexLCh <- function(langchain, OPENAI_API_KEY, path_save_index){
embeddings = langchain$embeddings$OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
db = langchain$vectorstores$FAISS$load_local(path_save_index, embeddings)
return(db)
}
searchLCh <- function(str, index, length_search_out = 5L){
data <- index$similarity_search_with_score(query = str, k = length_search_out) %>%
lapply(function(x) c(x[[1]]$metadata,
list(page_content = x[[1]]$page_content),
list(distance = x[[2]])) %>%
as.data.table()
) %>%
rbindlist(use.names = TRUE, fill = TRUE)
return(data)
}
index <- loadIndexLCh(langchain = langchain, OPENAI_API_KEY = OPENAI_API_KEY, path_save_index = PATH_SAVE_INDEX)
query <- "Эски квартал"
searchLCh(str = query, index = index, length_search_out = 15L)
query <- "lady"
data <- searchLCh(str = query, index = index, length_search_out = 15L)
data
query <- "lad"
data <- searchLCh(str = query, index = index, length_search_out = 15L)
data
